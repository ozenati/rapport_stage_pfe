After the panel factorization, the operation performed on the trailing sub-matrix is the update. The update takes the array of pivots produced by the panel factorization, swaps the rows and then apply \textit{trsm} and \textit{gemm} operations.

To perform one swap, the algorithm depend on the value of the pivots, this is what we call data dependency and so the algorithm is a dynamic algorithm (section \ref{task_flow_lu}). The solution to represent a dynamic algorithm with a static DAG of task flow is to make a path with all tasks. In practice, it means that all tiles must participate in swapping even if they are not concerned.
For that, each tile will have a workspace of two arrays: the first to store one row coming from the upper tile and the second to store one row going to the upper tile. Nodes will share between them the workspaces and fill them with the appropriate rows.
The \textit{all\_reduce} seems to be the right operation to use. The cost of a \textit{all\_reduce} operation is $log_2(n_t)$. Thus, the cost of all swaps of one panel is $n_b*log_2(n_t)$. This is very expensive relatively to the cost of SPMD model which is at most $2*n_b$.
Moreover, a the end of each \textit{all\_reduce} operation, only two nodes will really use the rows collected in their workspace (the upper tile and the tile which exchange with it).

A good idea is to perform all swaps at the same time. However, this is not possible with pivots. In fact, because the same row can be several time the maximum row, it is necessary to execute the pivots in the right order (from the first to the last). Figure \ref{fig:pivots} show an example of a row which contains successively two time the maximum row, we can see that the row 1 goes down to the row 13 then comes back to the row 2. Thus, it forces us to perform the first pivots before the second.

\begin{figure}[!ht]
\begin{minipage}[!ht]{.4\textwidth}
\centering
\includegraphics[height=5cm]{figures/pivots.pdf}
\caption{Row movements with pivots\label{fig:pivots}}
\end{minipage} \hfill
\begin{minipage}[!ht]{.4\textwidth}
\centering
\includegraphics[height=5cm]{figures/permutations.pdf}
\caption{Row movements with permutations\label{fig:permutations}}
\end{minipage}
\end{figure}

To reduce high cost of all swaps, the key is to use another structure instead of pivots. For that, the permutations are the right solution. In fact, permutations can be represented by an array of size $n$ (we see after that it can be reduced). For each index $x$ of the array $perm$, the row $perm(x)$ will be moved in place of the row $x$. Thus, with permutations, we know from the beginning the final place of each row. 
Figure \ref{fig:permutations} shows the use of permutations instead of pivots which is showed in Figure \ref{fig:pivots}, we can see that the row 1 goes directly to the row 2 and does	 not move again.
Thanks to this structure, all the rows can be swapped in one single step and the cost will be just $log_2(n_t)$. 
We remark that at most $n_b$ rows go into and from the diagonal tile. Thus, the array of permutations may be limited to a size of $2*n_b$ elements, the fist $nb$ elements will be used to store permutations and the second $nb$ elements will be used to store the inverse of permutations. Therefore, instead of using a workspace of two arrays, it is necessary to use two buffer - with size of tile - for communications: the first is a copy of the upper tile, it is shared from one node to the others. Each node will extracts from the copy rows that it needs. We will call this operation \emph{swap from}. The second buffer is used to gather rows required by the upper tile. Each node create its own buffer, fill it with rows intended to be stored in the upper tile and then participate with it in a \textit{gather} operation. This operation will be called \emph{swap into}. This solution allow us to perform the \emph{swap from} and the \emph{swap into} in parallel.

Task flow \ref{fig:distributed_update_task_flow} represents the swapping operation of update operation for distributed architecture. The bold arrows show some dependencies which add synchronizations to avoid \emph{read after write} effects. For example, the copy of the upper tile must be applied before its update.

\begin{taskflow}[!ht]
\centering
\includegraphics[width=0.8\textwidth]{figures/distributed_update_tf_bw.pdf}
\caption{Swapping operation of update on distributed architecture \label{fig:distributed_update_task_flow}}
\end{taskflow}

As for the panel factorization, we consider that nodes can be multi-core. In order to reduce global communication, each node share its buffers over its local tiles before to send them to others nodes. Task flow \ref{fig:update_task_flow} shows the update operation for hierarchical architecture.

\begin{taskflow}[!ht]
\centering
\includegraphics[width=0.8\textwidth]{figures/update_tf_bw.pdf}
\caption{Swapping operation of update on hierarchical architecture \label{fig:update_task_flow}}
\end{taskflow}

Moreover, this update algorithm implemented is a generic solution that it can execute update operation after any panel factorization which provide an array of pivots (incremental pivoting, CALU \dots).
