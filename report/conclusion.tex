LU decomposition with partial pivoting is a dynamic algorithm with its data dependencies in the swapping operation for the update. Despite this difficulty, we managed to implement a task flow LU decomposition with PTG and obtained hopeful performances.

We try now to optimize the \dague runtime system to better manage the small communications messages, in order to take full advantage of all cores of nodes. 
Thanks to this representation of the algorithm, we hope to create an algorithm that will fully exploit heterogeneous architectures by offloading GEMM to accelerators as GPUs or Intel MIC on distributed architectures. Nowadays, GPU implementations of the LU partial pivoting are doing the ScaLAPACK sequential execution of the algorithm and offload the GEMM on the GPU, or are able to fully exploit a single heterogeneous node by using block-column distribution. This would be a disaster in the load balance on distributed architectures where a 2D block-cyclic distribution is require to decrease the volume of communication and average out the load over the multiple nodes. 
%We will be able to have one of the most efficient LU decomposition implementation. Beside its use for resolving system of linear equations, it will be possible to create a new benchmark to analyse computers performances.

In parallel, we implemented another task flow LU decomposition on StarPU. In fact, StarPU runtime allow to implement dynamic algorithm  thanks to its task insertion system. Moreover, it allow to make \emph{reduce} operation which enable to select the best pivot without using tree implementation. We will be able to compare results to PTG in general and \dague particularly.